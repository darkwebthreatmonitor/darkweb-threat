# services/crawler/crawler_db.py
import sys
import requests
from bs4 import BeautifulSoup
from datetime import datetime

from api.db import SessionLocal, engine
from api.models import Base, Org, Query, CrawledPage

# Ensure tables exist (runs create_all). For production use migrations (Alembic).
Base.metadata.create_all(bind=engine)

def fetch_html(url: str, timeout: int = 20) -> (int, str):
    print("Fetching:", url)
    r = requests.get(url, timeout=timeout)
    r.raise_for_status()
    return r.status_code, r.text

def save_page_to_db(org_name: str, url: str, query_text: str = None):
    # org_name: human name like "acme-corp" (we'll find or create)
    db = SessionLocal()
    try:
        # find or create org
        org = db.query(Org).filter(Org.name == org_name).first()
        if not org:
            org = Org(name=org_name)
            db.add(org)
            db.commit()
            db.refresh(org)
            print("Created org:", org.id, org.name)

        # Optionally create a query row (for grouping crawls)
        q = None
        if query_text:
            q = Query(org_id=org.id, q_text=query_text, status="created")
            db.add(q)
            db.commit()
            db.refresh(q)
            print("Created query:", q.id, q.q_text)

        # fetch
        status_code, html = fetch_html(url)
        # preprocess minimally: prettify and snippet
        soup = BeautifulSoup(html, "html.parser")
        pretty = soup.prettify()
        snippet = soup.get_text()[:1000]  # first 1000 chars as preview

        # insert crawled page
        cp = CrawledPage(
            org_id=org.id,
            query_id=q.id if q else None,
            url=url,
            status_code=status_code,
            content=pretty,
            content_snippet=snippet,
            fetched_at=datetime.utcnow()
        )
        db.add(cp)
        db.commit()
        db.refresh(cp)
        print("Saved CrawledPage id=", cp.id, "org_id=", cp.org_id)
    except Exception:
        db.rollback()
        raise
    finally:
        db.close()

if __name__ == "__main__":
    # usage: python services/crawler/crawler_db.py <org_name> <url> [<query_text>]
    if len(sys.argv) < 3:
        print("Usage: python crawler_db.py <org_name> <url> [<query_text>]")
        sys.exit(1)
    org_name = sys.argv[1]
    url = sys.argv[2]
    query_text = sys.argv[3] if len(sys.argv) > 3 else None
    save_page_to_db(org_name, url, query_text)
